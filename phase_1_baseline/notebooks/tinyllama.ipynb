{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyLlama 自适应推理研究 Baseline 框架 (V2)\n",
    "\n",
    "**目标：** 建立一个可重复、可扩展的实验基线，用于评估和对比不同推理策略下的模型性能与效率。\n",
    "\n",
    "**V2 版本更新与修复：**\n",
    "\n",
    "1.  **修正 Prompt 模板 (关键修复)**：为 `TinyLlama-Chat` 模型应用了官方的聊天模板。这是导致 V1 版本准确率为 0 的核心原因。**对微调过的模型使用正确的对话模板至关重要**。\n",
    "2.  **修复量化模型加载错误**：调整了 `from_pretrained` 的 `device_map` 参数为 `\"auto\"`，解决了 4-bit 模型加载时与 `bitsandbytes` 的冲突。\n",
    "3.  **优化准确率评估**：`check_accuracy` 函数现在能更准确地解析模型输出的 'true'/'false' 或 'yes'/'no'，提高了评估的鲁棒性。\n",
    "4.  **依赖库补充**：在安装命令中加入了 `nbformat`，以解决 `wandb` 的警告信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阶段一：环境与配置准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的依赖库\n",
    "!pip install transformers==4.43.3 accelerate bitsandbytes sentencepiece datasets wandb thop torchprofile nbformat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\tinyllama\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3\\envs\\tinyllama\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.1\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from thop import profile\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 实验配置\n",
    "\n",
    "将所有可变参数集中管理，便于调试和记录。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspearcharlie04\u001b[0m (\u001b[33mspearcharlie04-hefei-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", # 使用Chat版本，更适合指令任务\n",
    "    \"dataset_name\": \"boolq\",                          # 使用BoolQ数据集，有明确的True/False标签，便于计算准确率\n",
    "    \"dataset_split\": \"validation\",\n",
    "    \"num_samples_to_evaluate\": 50,                      # 评估样本数，建议先用小数目快速迭代\n",
    "    \"max_new_tokens\": 10,                             # BoolQ 任务答案很短，不需要很多token\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"use_quantization\": False,                        # 是否使用4-bit量化加载\n",
    "    \"wandb_project\": \"adaptive-reasoning-baseline\",\n",
    "    \"wandb_run_name\": f\"baseline-v2-fp16-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "}\n",
    "\n",
    "# 登录 wandb (如果需要，取消注释并运行)\n",
    "# from google.colab import userdata\n",
    "# wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
    "try:\n",
    "    # 在本地环境中，通常会自动找到key\n",
    "    wandb.login()\n",
    "except Exception as e:\n",
    "    print(f\"Could not log in to wandb automatically: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阶段二：模型与数据加载（模块化）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, use_quantization, device):\n",
    "    \"\"\"加载模型和分词器，支持可选的4-bit量化。\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    quantization_config = None\n",
    "    # 只有在使用量化时才定义bnb_config\n",
    "    if use_quantization:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        # 关键修复：对于量化模型，必须使用 'auto' 让 accelerate 正确处理设备映射\n",
    "        device_map=\"auto\" if use_quantization else device,\n",
    "        torch_dtype=torch.bfloat16 if not use_quantization and torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def prepare_dataset(dataset_name, split, num_samples):\n",
    "    \"\"\"加载并准备数据集。\"\"\"\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    return dataset.shuffle(seed=42).select(range(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阶段三：核心评估框架\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(question, tokenizer):\n",
    "    \"\"\"关键修复：为聊天模型应用正确的指令模板。\"\"\"\n",
    "    # 这是 TinyLlama-Chat 期望的格式\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that answers questions with only 'true' or 'false'.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]\n",
    "    # `apply_chat_template` 会自动处理特殊 token，如 <|user|>, </s> 等\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def calculate_metrics(model, inputs):\n",
    "    \"\"\"计算单次前向传播的FLOPs和置信度（熵）。\"\"\"\n",
    "    flops, params = profile(model, inputs=(inputs[\"input_ids\"],), verbose=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=False, output_attentions=False)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1).item()\n",
    "    return flops, entropy\n",
    "\n",
    "def check_accuracy(generated_text, true_label):\n",
    "    \"\"\"优化后的准确率检查函数。\"\"\"\n",
    "    text = generated_text.lower().strip()\n",
    "    # 检查是否包含明确的肯定或否定词\n",
    "    is_true_pred = 'true' in text or 'yes' in text\n",
    "    is_false_pred = 'false' in text or 'no' in text\n",
    "    \n",
    "    if true_label:\n",
    "        return 1 if is_true_pred and not is_false_pred else 0\n",
    "    else:\n",
    "        return 1 if is_false_pred and not is_true_pred else 0\n",
    "\n",
    "def evaluate_baseline(model, tokenizer, dataset, config):\n",
    "    \"\"\"在数据集上评估模型，并记录所有关键指标。\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_latency, total_flops, total_entropy, total_accuracy = 0, 0, 0, 0\n",
    "    results = []\n",
    "\n",
    "    run = wandb.init(project=config[\"wandb_project\"], name=config[\"wandb_run_name\"], config=config, reinit=True)\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(dataset, desc=f\"Evaluating {config['wandb_run_name']}\")):\n",
    "        prompt = apply_chat_template(sample['question'], tokenizer)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        flops, entropy = calculate_metrics(model, inputs)\n",
    "        total_flops += flops\n",
    "        total_entropy += entropy\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=config[\"max_new_tokens\"], pad_token_id=tokenizer.pad_token_id)\n",
    "        torch.cuda.synchronize()\n",
    "        latency = time.time() - start_time\n",
    "        total_latency += latency\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer_text = generated_text[len(prompt):].strip()\n",
    "        accuracy = check_accuracy(answer_text, sample['answer'])\n",
    "        total_accuracy += accuracy\n",
    "        \n",
    "        step_metrics = {\"latency\": latency, \"flops_G\": flops / 1e9, \"entropy\": entropy, \"accuracy\": accuracy}\n",
    "        wandb.log(step_metrics, step=i)\n",
    "        results.append({\"prompt\": prompt, \"generated_answer\": answer_text, \"true_answer\": sample['answer'], **step_metrics})\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "    summary_metrics = {\n",
    "        \"avg_latency_sec\": total_latency / num_samples,\n",
    "        \"avg_flops_G\": (total_flops / num_samples) / 1e9,\n",
    "        \"avg_entropy\": total_entropy / num_samples,\n",
    "        \"final_accuracy\": total_accuracy / num_samples\n",
    "    }\n",
    "    wandb.summary.update(summary_metrics)\n",
    "    wandb.finish()\n",
    "    \n",
    "    print(f\"\\n--- {config['wandb_run_name']} Evaluation Summary ---\")\n",
    "    for key, value in summary_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "        \n",
    "    return results, summary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阶段四：运行实验并分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 加载数据 ---\n",
    "dataset = prepare_dataset(CONFIG[\"dataset_name\"], CONFIG[\"dataset_split\"], CONFIG[\"num_samples_to_evaluate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline with FP16/BF16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\陈璟\\Desktop\\Research\\AR\\wandb\\run-20251021_074856-8g9od9zf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline/runs/8g9od9zf' target=\"_blank\">baseline-v2-fp16-20251021-074849</a></strong> to <a href='https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline' target=\"_blank\">https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline/runs/8g9od9zf' target=\"_blank\">https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline/runs/8g9od9zf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f504e6f1152d4600a2463901ec007020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating baseline-v2-fp16-20251021-074849:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy</td><td>▅▆▃▃▃▅▃▄▃▅▄▂▃▄▃▄▇▃█▄▃█▆▅▅▂▅▇▃▁▆▅▂▄▅▃▅▅▅▆</td></tr><tr><td>flops_G</td><td>▃▄▁▂▆▂▂▂▃▁▄▇▅▇▅▄▄▅▂█▄▁▃▃▄▄▁▆▂▆▃▄▄▅▂▂▅▃▄▄</td></tr><tr><td>latency</td><td>▆▂▃▄▅▃▂▆▄▁▂▄▂▂▂▃▂▃▂▂▂▂▁▂▂▁▇█▇▂▆▂▃▂▇▁▁▂▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0</td></tr><tr><td>avg_entropy</td><td>1.42216</td></tr><tr><td>avg_flops_G</td><td>55.58974</td></tr><tr><td>avg_latency_sec</td><td>0.33736</td></tr><tr><td>entropy</td><td>1.71769</td></tr><tr><td>final_accuracy</td><td>0.08</td></tr><tr><td>flops_G</td><td>55.85869</td></tr><tr><td>latency</td><td>0.28294</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline-v2-fp16-20251021-074849</strong> at: <a href='https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline/runs/8g9od9zf' target=\"_blank\">https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline/runs/8g9od9zf</a><br> View project at: <a href='https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline' target=\"_blank\">https://wandb.ai/spearcharlie04-hefei-university-of-technology/adaptive-reasoning-baseline</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251021_074856-8g9od9zf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- baseline-v2-fp16-20251021-074849 Evaluation Summary ---\n",
      "avg_latency_sec: 0.3374\n",
      "avg_flops_G: 55.5897\n",
      "avg_entropy: 1.4222\n",
      "final_accuracy: 0.0800\n"
     ]
    }
   ],
   "source": [
    "# --- 运行 FP16/BF16 基线 ---\n",
    "print(\"Running baseline with FP16/BF16...\")\n",
    "CONFIG[\"use_quantization\"] = False\n",
    "CONFIG[\"wandb_run_name\"] = f\"baseline-v2-fp16-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(CONFIG[\"model_id\"], CONFIG[\"use_quantization\"], CONFIG[\"device\"])\n",
    "\n",
    "fp16_results, fp16_summary = evaluate_baseline(model, tokenizer, dataset, CONFIG)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running baseline with 4-bit Quantization...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb_run_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline-v2-int4-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 注意：我们传递 `device='auto'`，但函数内部会为量化模型覆盖它\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model_q, tokenizer_q \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_quantization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m int4_results, int4_summary \u001b[38;5;241m=\u001b[39m evaluate_baseline(model_q, tokenizer_q, dataset, CONFIG)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model_q\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[1;34m(model_id, use_quantization, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_quantization:\n\u001b[0;32m     10\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     11\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[0;32m     15\u001b[0m     )\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 关键修复：对于量化模型，必须使用 'auto' 让 accelerate 正确处理设备映射\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_quantization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_quantization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\tinyllama\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\tinyllama\\lib\\site-packages\\transformers\\modeling_utils.py:3990\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3987\u001b[0m         device_map_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_buffers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[1;32m-> 3990\u001b[0m         dispatch_model(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   3992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3993\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mpostprocess_model(model)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\tinyllama\\lib\\site-packages\\accelerate\\big_modeling.py:502\u001b[0m, in \u001b[0;36mdispatch_model\u001b[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[0;32m    500\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 502\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    506\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\tinyllama\\lib\\site-packages\\transformers\\modeling_utils.py:2839\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2837\u001b[0m \u001b[38;5;66;03m# Checks if the model has been loaded in 8-bit\u001b[39;00m\n\u001b[0;32m   2838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES:\n\u001b[1;32m-> 2839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2842\u001b[0m     )\n\u001b[0;32m   2843\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ:\n\u001b[0;32m   2844\u001b[0m     \u001b[38;5;66;03m# For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\u001b[39;00m\n\u001b[0;32m   2845\u001b[0m     \u001b[38;5;66;03m# the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m     dtype_present_in_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "# --- 运行 4-bit 量化基线 ---\n",
    "print(\"\\nRunning baseline with 4-bit Quantization...\")\n",
    "CONFIG[\"use_quantization\"] = True\n",
    "CONFIG[\"wandb_run_name\"] = f\"baseline-v2-int4-{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# 注意：我们传递 `device='auto'`，但函数内部会为量化模型覆盖它\n",
    "model_q, tokenizer_q = load_model_and_tokenizer(CONFIG[\"model_id\"], CONFIG[\"use_quantization\"], CONFIG[\"device\"])\n",
    "\n",
    "int4_results, int4_summary = evaluate_baseline(model_q, tokenizer_q, dataset, CONFIG)\n",
    "\n",
    "del model_q\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 阶段五：结论与下一步\n",
    "\n",
    "现在，V2 版本的基线应该可以正常运行并报告有意义的准确率了。FP16 和 INT4 的结果将为你提供一个坚实的、可信的性能和效率基准。\n",
    "\n",
    "**下一步行动 (Action Plan):**\n",
    "\n",
    "你的研究计划现在可以真正进入 **阶段二：模型改进与实现**。\n",
    "\n",
    "1.  **复制并修改评估函数**：创建一个 `evaluate_adaptive` 函数。\n",
    "2.  **实现你的核心创新**：在这个新函数中，用一个自定义的生成循环替换 `model.generate()`。在这个循环里，你将逐个 token 生成，并在每一步之后计算熵。当熵低于某个阈值 `τ` 时，就提前终止生成。\n",
    "3.  **开始对比实验**：设置不同的熵阈值（例如 `τ = [0.2, 0.5, 1.0]`）进行实验，并将结果与 V2 基线进行比较，绘制**准确率-计算成本（FLOPs/Latency）的权衡曲线**。这很可能就是你论文中的核心图表。\n",
    "\n",
    "你已经解决了初始的技术障碍，现在可以专注于算法创新了。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
